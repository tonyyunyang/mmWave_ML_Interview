{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import lime\n",
    "import os\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "class ActivityXAIAnalyzer:\n",
    "    def __init__(self, X, cluster_labels):\n",
    "        \"\"\"\n",
    "        Initialize the XAI analyzer with data and cluster labels\n",
    "        \n",
    "        Parameters:\n",
    "        X (array-like): Feature matrix from micro-Doppler signatures\n",
    "        cluster_labels (array-like): Cluster assignments from part (a)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.cluster_labels = cluster_labels\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_scaled = self.scaler.fit_transform(X)\n",
    "        self.rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.rf_model.fit(self.X_scaled, self.cluster_labels)\n",
    "        \n",
    "    def analyze_shap_values(self):\n",
    "        \"\"\"\n",
    "        Use SHAP values to explain feature importance for each cluster\n",
    "        \"\"\"\n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.TreeExplainer(self.rf_model)\n",
    "        shap_values = explainer.shap_values(self.X_scaled)\n",
    "        \n",
    "        # For each cluster, get the most important features\n",
    "        feature_importance_per_cluster = {}\n",
    "        for cluster_idx in range(len(np.unique(self.cluster_labels))):\n",
    "            # Get mean absolute SHAP values for this cluster\n",
    "            cluster_importance = np.abs(shap_values[cluster_idx]).mean(0)\n",
    "            # Get top features indices\n",
    "            top_features_idx = np.argsort(cluster_importance)[-5:]  # Top 5 features\n",
    "            feature_importance_per_cluster[f'Cluster_{cluster_idx}'] = {\n",
    "                'indices': top_features_idx,\n",
    "                'importance_values': cluster_importance[top_features_idx]\n",
    "            }\n",
    "        \n",
    "        return feature_importance_per_cluster\n",
    "    \n",
    "    def predict_proba_wrapper(self, X):\n",
    "        \"\"\"\n",
    "        Wrapper for model predictions to ensure correct format for LIME\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        return self.rf_model.predict_proba(X)\n",
    "    \n",
    "    def analyze_lime(self, sample_indices=None):\n",
    "        \"\"\"\n",
    "        Use LIME to explain individual instances from each cluster\n",
    "        \"\"\"\n",
    "        if sample_indices is None:\n",
    "            # Randomly select one sample from each cluster\n",
    "            sample_indices = []\n",
    "            for cluster in np.unique(self.cluster_labels):\n",
    "                cluster_samples = np.where(self.cluster_labels == cluster)[0]\n",
    "                sample_indices.append(np.random.choice(cluster_samples))\n",
    "        \n",
    "        # Initialize LIME explainer\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "            self.X_scaled,\n",
    "            mode='classification',\n",
    "            feature_names=[f'feature_{i}' for i in range(self.X.shape[1])],\n",
    "            class_names=[f'Cluster_{i}' for i in range(len(np.unique(self.cluster_labels)))]\n",
    "        )\n",
    "        \n",
    "        lime_explanations = {}\n",
    "        for idx in sample_indices:\n",
    "            explanation = explainer.explain_instance(\n",
    "                self.X_scaled[idx], \n",
    "                self.predict_proba_wrapper,\n",
    "                num_features=5  # Explain top 5 features\n",
    "            )\n",
    "            lime_explanations[f'Sample_{idx}'] = explanation.as_list()\n",
    "            \n",
    "        return lime_explanations\n",
    "    \n",
    "    def analyze_permutation_importance(self):\n",
    "        \"\"\"\n",
    "        Calculate permutation importance for features\n",
    "        \"\"\"\n",
    "        result = permutation_importance(\n",
    "            self.rf_model, self.X_scaled, self.cluster_labels,\n",
    "            n_repeats=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'importances_mean': result.importances_mean,\n",
    "            'importances_std': result.importances_std,\n",
    "            'top_features': np.argsort(result.importances_mean)[-5:]  # Top 5 features\n",
    "        }\n",
    "    \n",
    "    def get_consensus_features(self):\n",
    "        \"\"\"\n",
    "        Combine results from all methods to get consensus important features\n",
    "        \"\"\"\n",
    "        shap_results = self.analyze_shap_values()\n",
    "        lime_results = self.analyze_lime()\n",
    "        perm_results = self.analyze_permutation_importance()\n",
    "        \n",
    "        # Combine and analyze overlap between methods\n",
    "        consensus_features = {}\n",
    "        for cluster in np.unique(self.cluster_labels):\n",
    "            cluster_key = f'Cluster_{cluster}'\n",
    "            \n",
    "            # Get important features from each method\n",
    "            shap_features = set(shap_results[cluster_key]['indices'])\n",
    "            perm_features = set(perm_results['top_features'])\n",
    "            \n",
    "            # Find features that appear in multiple methods\n",
    "            consensus = shap_features.intersection(perm_features)\n",
    "            consensus_features[cluster_key] = list(consensus)\n",
    "            \n",
    "        return consensus_features\n",
    "\n",
    "    def visualize_feature_importance(self, method='shap'):\n",
    "        \"\"\"\n",
    "        Visualize feature importance for each cluster\n",
    "        \"\"\"\n",
    "        if method == 'shap':\n",
    "            results = self.analyze_shap_values()\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for cluster, data in results.items():\n",
    "                plt.bar(\n",
    "                    [f'Feature_{i}' for i in data['indices']],\n",
    "                    data['importance_values'],\n",
    "                    label=cluster\n",
    "                )\n",
    "            plt.title('SHAP Feature Importance by Cluster')\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('Importance')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_xai(base_path='data/a_cluster'):\n",
    "    \"\"\"\n",
    "    Load and format data for XAI analysis\n",
    "    \n",
    "    Parameters:\n",
    "    base_path (str): Path to the cluster directory\n",
    "    \n",
    "    Returns:\n",
    "    X (np.array): Feature matrix where each row is a sample\n",
    "    cluster_labels (np.array): Cluster labels for each sample\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Get all cluster directories\n",
    "    cluster_dirs = sorted([d for d in os.listdir(base_path) if d.startswith('cluster_')])\n",
    "    \n",
    "    for cluster_idx, cluster_dir in enumerate(cluster_dirs):\n",
    "        cluster_path = os.path.join(base_path, cluster_dir)\n",
    "        \n",
    "        if os.path.isdir(cluster_path):\n",
    "            # Get all .npy files in the cluster directory\n",
    "            npy_files = sorted([f for f in os.listdir(cluster_path) if f.endswith('.npy')])\n",
    "            \n",
    "            for npy_file in npy_files:\n",
    "                file_path = os.path.join(cluster_path, npy_file)\n",
    "                data = np.load(file_path)\n",
    "                data = data[:409, :]\n",
    "                # Flatten or reshape data if needed\n",
    "                data_flat = data.flatten()  # or use appropriate reshaping\n",
    "                all_samples.append(data_flat)\n",
    "                all_labels.append(cluster_idx)\n",
    "    \n",
    "    X = np.array(all_samples)\n",
    "    cluster_labels = np.array(all_labels)\n",
    "    \n",
    "    return X, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your feature matrix and cluster_labels are from part (a)\n",
    "X, cluster_labels = load_data_for_xai()\n",
    "xai_analyzer = ActivityXAIAnalyzer(X, cluster_labels)\n",
    "\n",
    "# Get consensus important features\n",
    "important_features = xai_analyzer.get_consensus_features()\n",
    "\n",
    "# Visualize results\n",
    "xai_analyzer.visualize_feature_importance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
